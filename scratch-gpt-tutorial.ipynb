{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlT21KutT5WzE/eKpsxs5G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Scratch GPT Tutorial\n","Following along to Andrej Karpathy's GPT from scratch video. I am using the ASV Bible instead of Shakespeare for my data just for fun."],"metadata":{"id":"f47Aynk0_Lfx"}},{"cell_type":"code","source":["!wget https://openbible.com/textfiles/asv.txt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yfkjm3KP0apo","executionInfo":{"status":"ok","timestamp":1691305056676,"user_tz":300,"elapsed":757,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"584c7d7f-b64c-44fd-8e9f-fbcb04beca59"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["--2023-08-06 06:57:35--  https://openbible.com/textfiles/asv.txt\n","Resolving openbible.com (openbible.com)... 74.63.245.138\n","Connecting to openbible.com (openbible.com)|74.63.245.138|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4568028 (4.4M) [text/plain]\n","Saving to: ‘asv.txt’\n","\n","asv.txt             100%[===================>]   4.36M  14.4MB/s    in 0.3s    \n","\n","2023-08-06 06:57:36 (14.4 MB/s) - ‘asv.txt’ saved [4568028/4568028]\n","\n"]}]},{"cell_type":"code","source":["with open('asv.txt', 'r', encoding='utf-8') as f:\n","  text = f.read()"],"metadata":{"id":"K4Cj211M0k-W","executionInfo":{"status":"ok","timestamp":1691305061169,"user_tz":300,"elapsed":140,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["print(\"length of dataset in characters: \", len(text))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KgkGbRyf05L3","executionInfo":{"status":"ok","timestamp":1691305064436,"user_tz":300,"elapsed":211,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"7d0ee73d-773c-4b80-8f20-fd43d0758cc2"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["length of dataset in characters:  4563817\n"]}]},{"cell_type":"code","source":["print(text[:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sm57haFu1Axu","executionInfo":{"status":"ok","timestamp":1691305066258,"user_tz":300,"elapsed":143,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"2567b7ac-db28-4ce1-aa52-373bd5c76f4c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["﻿ASV\n","American Standard Version\n","Genesis 1:1\tIn the beginning God created the heavens and the earth.\n","Genesis 1:2\tAnd the earth was waste and void; and darkness was upon the face of the deep: and the Spirit of God moved upon the face of the waters.\n","Genesis 1:3\tAnd God said, Let there be light: and there was light.\n","Genesis 1:4\tAnd God saw the light, that it was good: and God divided the light from the darkness.\n","Genesis 1:5\tAnd God called the light Day, and the darkness he called Night. And there was evening and there was morning, one day.\n","Genesis 1:6\tAnd God said, Let there be a firmament in the midst of the waters, and let it divide the waters from the waters.\n","Genesis 1:7\tAnd God made the firmament, and divided the waters which were under the firmament from the waters which were above the firmament: and it was so.\n","Genesis 1:8\tAnd God called the firmament Heaven. And there was evening and there was morning, a second day.\n","Genesis 1:9\tAnd God said, Let the waters under the heavens be gathere\n"]}]},{"cell_type":"code","source":["chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","print(''.join(chars))\n","print(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pYIxQQ731JGF","executionInfo":{"status":"ok","timestamp":1691305068418,"user_tz":300,"elapsed":124,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"ee2a05a9-7b4c-4b1e-f162-ade09ecbec77"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["\t\n"," !(),-.0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWYZ[]abcdefghijklmnopqrstuvwxyzÆæ—’﻿\n","80\n"]}]},{"cell_type":"code","source":["stoi = { ch:i for i,ch in enumerate(chars) } # mappings\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encodings\n","decode = lambda l: ''.join([itos[i] for i in l])\n","\n","print(encode(\"hello there\"))\n","print(decode(encode(\"hello there\")))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_DyDA_j31MV6","executionInfo":{"status":"ok","timestamp":1691305070065,"user_tz":300,"elapsed":150,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"34a3c719-9442-496d-9580-e6735b529735"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["[56, 53, 60, 60, 63, 2, 68, 56, 53, 66, 53]\n","hello there\n"]}]},{"cell_type":"code","source":["import torch\n","data = torch.tensor(encode(text), dtype=torch.long)\n","print(data.shape, data.dtype)\n","print(data[:1000])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RCDgE4lQ2X1T","executionInfo":{"status":"ok","timestamp":1691305079586,"user_tz":300,"elapsed":7704,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"7b704c55-f59a-4dd7-8578-edddfb62517f"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([4563817]) torch.int64\n","tensor([79, 22, 40, 43,  1, 22, 61, 53, 66, 57, 51, 49, 62,  2, 40, 68, 49, 62,\n","        52, 49, 66, 52,  2, 43, 53, 66, 67, 57, 63, 62,  1, 28, 53, 62, 53, 67,\n","        57, 67,  2, 10, 19, 10,  0, 30, 62,  2, 68, 56, 53,  2, 50, 53, 55, 57,\n","        62, 62, 57, 62, 55,  2, 28, 63, 52,  2, 51, 66, 53, 49, 68, 53, 52,  2,\n","        68, 56, 53,  2, 56, 53, 49, 70, 53, 62, 67,  2, 49, 62, 52,  2, 68, 56,\n","        53,  2, 53, 49, 66, 68, 56,  8,  1, 28, 53, 62, 53, 67, 57, 67,  2, 10,\n","        19, 11,  0, 22, 62, 52,  2, 68, 56, 53,  2, 53, 49, 66, 68, 56,  2, 71,\n","        49, 67,  2, 71, 49, 67, 68, 53,  2, 49, 62, 52,  2, 70, 63, 57, 52, 20,\n","         2, 49, 62, 52,  2, 52, 49, 66, 59, 62, 53, 67, 67,  2, 71, 49, 67,  2,\n","        69, 64, 63, 62,  2, 68, 56, 53,  2, 54, 49, 51, 53,  2, 63, 54,  2, 68,\n","        56, 53,  2, 52, 53, 53, 64, 19,  2, 49, 62, 52,  2, 68, 56, 53,  2, 40,\n","        64, 57, 66, 57, 68,  2, 63, 54,  2, 28, 63, 52,  2, 61, 63, 70, 53, 52,\n","         2, 69, 64, 63, 62,  2, 68, 56, 53,  2, 54, 49, 51, 53,  2, 63, 54,  2,\n","        68, 56, 53,  2, 71, 49, 68, 53, 66, 67,  8,  1, 28, 53, 62, 53, 67, 57,\n","        67,  2, 10, 19, 12,  0, 22, 62, 52,  2, 28, 63, 52,  2, 67, 49, 57, 52,\n","         6,  2, 33, 53, 68,  2, 68, 56, 53, 66, 53,  2, 50, 53,  2, 60, 57, 55,\n","        56, 68, 19,  2, 49, 62, 52,  2, 68, 56, 53, 66, 53,  2, 71, 49, 67,  2,\n","        60, 57, 55, 56, 68,  8,  1, 28, 53, 62, 53, 67, 57, 67,  2, 10, 19, 13,\n","         0, 22, 62, 52,  2, 28, 63, 52,  2, 67, 49, 71,  2, 68, 56, 53,  2, 60,\n","        57, 55, 56, 68,  6,  2, 68, 56, 49, 68,  2, 57, 68,  2, 71, 49, 67,  2,\n","        55, 63, 63, 52, 19,  2, 49, 62, 52,  2, 28, 63, 52,  2, 52, 57, 70, 57,\n","        52, 53, 52,  2, 68, 56, 53,  2, 60, 57, 55, 56, 68,  2, 54, 66, 63, 61,\n","         2, 68, 56, 53,  2, 52, 49, 66, 59, 62, 53, 67, 67,  8,  1, 28, 53, 62,\n","        53, 67, 57, 67,  2, 10, 19, 14,  0, 22, 62, 52,  2, 28, 63, 52,  2, 51,\n","        49, 60, 60, 53, 52,  2, 68, 56, 53,  2, 60, 57, 55, 56, 68,  2, 25, 49,\n","        73,  6,  2, 49, 62, 52,  2, 68, 56, 53,  2, 52, 49, 66, 59, 62, 53, 67,\n","        67,  2, 56, 53,  2, 51, 49, 60, 60, 53, 52,  2, 35, 57, 55, 56, 68,  8,\n","         2, 22, 62, 52,  2, 68, 56, 53, 66, 53,  2, 71, 49, 67,  2, 53, 70, 53,\n","        62, 57, 62, 55,  2, 49, 62, 52,  2, 68, 56, 53, 66, 53,  2, 71, 49, 67,\n","         2, 61, 63, 66, 62, 57, 62, 55,  6,  2, 63, 62, 53,  2, 52, 49, 73,  8,\n","         1, 28, 53, 62, 53, 67, 57, 67,  2, 10, 19, 15,  0, 22, 62, 52,  2, 28,\n","        63, 52,  2, 67, 49, 57, 52,  6,  2, 33, 53, 68,  2, 68, 56, 53, 66, 53,\n","         2, 50, 53,  2, 49,  2, 54, 57, 66, 61, 49, 61, 53, 62, 68,  2, 57, 62,\n","         2, 68, 56, 53,  2, 61, 57, 52, 67, 68,  2, 63, 54,  2, 68, 56, 53,  2,\n","        71, 49, 68, 53, 66, 67,  6,  2, 49, 62, 52,  2, 60, 53, 68,  2, 57, 68,\n","         2, 52, 57, 70, 57, 52, 53,  2, 68, 56, 53,  2, 71, 49, 68, 53, 66, 67,\n","         2, 54, 66, 63, 61,  2, 68, 56, 53,  2, 71, 49, 68, 53, 66, 67,  8,  1,\n","        28, 53, 62, 53, 67, 57, 67,  2, 10, 19, 16,  0, 22, 62, 52,  2, 28, 63,\n","        52,  2, 61, 49, 52, 53,  2, 68, 56, 53,  2, 54, 57, 66, 61, 49, 61, 53,\n","        62, 68,  6,  2, 49, 62, 52,  2, 52, 57, 70, 57, 52, 53, 52,  2, 68, 56,\n","        53,  2, 71, 49, 68, 53, 66, 67,  2, 71, 56, 57, 51, 56,  2, 71, 53, 66,\n","        53,  2, 69, 62, 52, 53, 66,  2, 68, 56, 53,  2, 54, 57, 66, 61, 49, 61,\n","        53, 62, 68,  2, 54, 66, 63, 61,  2, 68, 56, 53,  2, 71, 49, 68, 53, 66,\n","        67,  2, 71, 56, 57, 51, 56,  2, 71, 53, 66, 53,  2, 49, 50, 63, 70, 53,\n","         2, 68, 56, 53,  2, 54, 57, 66, 61, 49, 61, 53, 62, 68, 19,  2, 49, 62,\n","        52,  2, 57, 68,  2, 71, 49, 67,  2, 67, 63,  8,  1, 28, 53, 62, 53, 67,\n","        57, 67,  2, 10, 19, 17,  0, 22, 62, 52,  2, 28, 63, 52,  2, 51, 49, 60,\n","        60, 53, 52,  2, 68, 56, 53,  2, 54, 57, 66, 61, 49, 61, 53, 62, 68,  2,\n","        29, 53, 49, 70, 53, 62,  8,  2, 22, 62, 52,  2, 68, 56, 53, 66, 53,  2,\n","        71, 49, 67,  2, 53, 70, 53, 62, 57, 62, 55,  2, 49, 62, 52,  2, 68, 56,\n","        53, 66, 53,  2, 71, 49, 67,  2, 61, 63, 66, 62, 57, 62, 55,  6,  2, 49,\n","         2, 67, 53, 51, 63, 62, 52,  2, 52, 49, 73,  8,  1, 28, 53, 62, 53, 67,\n","        57, 67,  2, 10, 19, 18,  0, 22, 62, 52,  2, 28, 63, 52,  2, 67, 49, 57,\n","        52,  6,  2, 33, 53, 68,  2, 68, 56, 53,  2, 71, 49, 68, 53, 66, 67,  2,\n","        69, 62, 52, 53, 66,  2, 68, 56, 53,  2, 56, 53, 49, 70, 53, 62, 67,  2,\n","        50, 53,  2, 55, 49, 68, 56, 53, 66, 53])\n"]}]},{"cell_type":"code","source":["# partitioning data to prevent model from becoming an exact memorization of input data\n","n = int(0.9*len(data))\n","train_data = data[:n]\n","val_data = data[n:]"],"metadata":{"id":"5s4sdPF33atW","executionInfo":{"status":"ok","timestamp":1691305083531,"user_tz":300,"elapsed":199,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["block_size = 8\n","train_data[:block_size+1]"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AHZBhhoH4RGx","executionInfo":{"status":"ok","timestamp":1691305117452,"user_tz":300,"elapsed":210,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"e1770e87-757e-45f3-b5ca-1677f0c1c7f4"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([79, 22, 40, 43,  1, 22, 61, 53, 66])"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["x = train_data[:block_size]\n","y = train_data[1:block_size+1]\n","for t in range(block_size):\n","  context = x[:t+1]\n","  target = y[t]\n","  print(f\"when input is {context} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6kzAbXGb4zyE","executionInfo":{"status":"ok","timestamp":1691305119031,"user_tz":300,"elapsed":140,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"3ab5c73f-be84-4ed9-da67-8dc8ccafc795"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["when input is tensor([79]) the target: 22\n","when input is tensor([79, 22]) the target: 40\n","when input is tensor([79, 22, 40]) the target: 43\n","when input is tensor([79, 22, 40, 43]) the target: 1\n","when input is tensor([79, 22, 40, 43,  1]) the target: 22\n","when input is tensor([79, 22, 40, 43,  1, 22]) the target: 61\n","when input is tensor([79, 22, 40, 43,  1, 22, 61]) the target: 53\n","when input is tensor([79, 22, 40, 43,  1, 22, 61, 53]) the target: 66\n"]}]},{"cell_type":"code","source":["batch_size = 4 # number of parallel sequences processed\n","block_size = 8 # max context length for predictions\n","\n","def get_batch(split): # creates batch of input data (x) and target data (y)\n","  data = train_data if split == \"train\" else val_data\n","  ix = torch.randint(len(data) - block_size, (batch_size,))\n","  x = torch.stack([data[i:i+block_size] for i in ix])\n","  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","  return x,y\n","\n","xb,yb = get_batch('train')\n","print('inputs:')\n","print(xb.shape)\n","print(xb)\n","print('targets:')\n","print(yb.shape)\n","print(yb)\n","\n","print('----')\n","\n","for b in range(batch_size): # batch dimension\n","  for t in range(block_size): # time dimension\n","    context = xb[b, :t+1]\n","    target = yb[b,t]\n","    print(f\"when input is {context.tolist()} the target: {target}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tsnuig9B5gHL","executionInfo":{"status":"ok","timestamp":1691305121007,"user_tz":300,"elapsed":126,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"a123958f-7369-44d2-835b-00b12056982d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:\n","torch.Size([4, 8])\n","tensor([[ 6,  2, 44, 53,  2, 56, 49, 70],\n","        [62, 52,  2, 51, 56, 49, 66, 55],\n","        [ 2, 49, 60, 60,  2, 68, 56, 53],\n","        [63,  2, 68, 56, 53,  2, 71, 49]])\n","targets:\n","torch.Size([4, 8])\n","tensor([[ 2, 44, 53,  2, 56, 49, 70, 53],\n","        [52,  2, 51, 56, 49, 66, 55, 53],\n","        [49, 60, 60,  2, 68, 56, 53, 67],\n","        [ 2, 68, 56, 53,  2, 71, 49, 60]])\n","----\n","when input is [6] the target: 2\n","when input is [6, 2] the target: 44\n","when input is [6, 2, 44] the target: 53\n","when input is [6, 2, 44, 53] the target: 2\n","when input is [6, 2, 44, 53, 2] the target: 56\n","when input is [6, 2, 44, 53, 2, 56] the target: 49\n","when input is [6, 2, 44, 53, 2, 56, 49] the target: 70\n","when input is [6, 2, 44, 53, 2, 56, 49, 70] the target: 53\n","when input is [62] the target: 52\n","when input is [62, 52] the target: 2\n","when input is [62, 52, 2] the target: 51\n","when input is [62, 52, 2, 51] the target: 56\n","when input is [62, 52, 2, 51, 56] the target: 49\n","when input is [62, 52, 2, 51, 56, 49] the target: 66\n","when input is [62, 52, 2, 51, 56, 49, 66] the target: 55\n","when input is [62, 52, 2, 51, 56, 49, 66, 55] the target: 53\n","when input is [2] the target: 49\n","when input is [2, 49] the target: 60\n","when input is [2, 49, 60] the target: 60\n","when input is [2, 49, 60, 60] the target: 2\n","when input is [2, 49, 60, 60, 2] the target: 68\n","when input is [2, 49, 60, 60, 2, 68] the target: 56\n","when input is [2, 49, 60, 60, 2, 68, 56] the target: 53\n","when input is [2, 49, 60, 60, 2, 68, 56, 53] the target: 67\n","when input is [63] the target: 2\n","when input is [63, 2] the target: 68\n","when input is [63, 2, 68] the target: 56\n","when input is [63, 2, 68, 56] the target: 53\n","when input is [63, 2, 68, 56, 53] the target: 2\n","when input is [63, 2, 68, 56, 53, 2] the target: 71\n","when input is [63, 2, 68, 56, 53, 2, 71] the target: 49\n","when input is [63, 2, 68, 56, 53, 2, 71, 49] the target: 60\n"]}]},{"cell_type":"code","source":["print(xb) # this is the input batch"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ScqZgwg_puJ","executionInfo":{"status":"ok","timestamp":1691305125336,"user_tz":300,"elapsed":158,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"98e4425a-0373-4151-a449-3fac21c7e381"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 6,  2, 44, 53,  2, 56, 49, 70],\n","        [62, 52,  2, 51, 56, 49, 66, 55],\n","        [ 2, 49, 60, 60,  2, 68, 56, 53],\n","        [63,  2, 68, 56, 53,  2, 71, 49]])\n"]}]},{"cell_type":"code","source":["# starting with a very simple language model, the bigram language model\n","import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","class BigramLanguageModel(nn.Module):\n","\n","  def __init__(self, vocab_size):\n","    super().__init__()\n","    self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # look up table mapping tokens to the logits (probabilities) for the next token\n","\n","  def forward(self, idx, targets=None): # ends up getting called when the object is called\n","    # idx and targets are 2-dimensional (B,T) tensors\n","    logits = self.token_embedding_table(idx) # look up the logits for the next token, dimensions B,T,C (batch_size,time,channels/vocab_size)\n","\n","    if targets is None:\n","      loss = None\n","    else:\n","      B, T, C = logits.shape\n","      logits = logits.view(B*T, C) # reshape logits to 2-dimensions\n","      targets = targets.view(B*T) # reshape targets to 1-dimension\n","      loss = F.cross_entropy(logits, targets) # the quality of the predictions, expects C to be second dimension if multidimensional tensor\n","\n","    return logits, loss\n","\n","  def generate(self, idx, max_new_tokens):\n","    # idx is 2-dimensional (B,T) in this context\n","    for _ in range(max_new_tokens):\n","      logits, loss = self(idx) # get predictions\n","      logits = logits[:, -1, :] # look only at the last time step, becomes (B,C)\n","      probs = F.softmax(logits, dim=-1) # get probabilities\n","      idx_next = torch.multinomial(probs, num_samples=1) # sample the distribution for a single prediction in the time dimension in each batch dimension, (B,1)\n","      idx = torch.cat((idx, idx_next), dim=1) # append sample to the sequence, becomes (B,T+1)\n","    return idx\n","\n","\n","m = BigramLanguageModel(vocab_size)\n","logits, loss = m(xb,yb)\n","print(logits.shape)\n","print(loss) # we expect loss of -ln(1/vocab_size) for decent performance\n","\n","idx = torch.zeros((1,1), dtype=torch.long) # creates a 1x1 tensor containing a zero, which I think is representinga tab character\n","out = m.generate(idx, max_new_tokens=100)[0].tolist() # generates 100 tokens based on that\n","print(decode(out)) # print the decoded generation, will be garbage because the model is completely random, not trained yet"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZxYKc1jw_w77","executionInfo":{"status":"ok","timestamp":1691305127085,"user_tz":300,"elapsed":394,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"b55eef3f-e43c-4cb1-d7c3-dc94f3a3d0a8"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([32, 80])\n","tensor(5.0331, grad_fn=<NllLossBackward0>)\n","\txN\n","J;AKGfR[;bfCUiMI:hixAE﻿ey)EYlHuf9)Qe)ARi;2Akp[f9V’(.\t.qxb-dG?M!.D  0qE65, FGb73yQ2pdVyQ5LuNæ7guIL\n"]}]},{"cell_type":"code","source":["optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"],"metadata":{"id":"BslQyTMoJiHn","executionInfo":{"status":"ok","timestamp":1691305129314,"user_tz":300,"elapsed":2,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["batch_size = 32 # try using a bigger batch size\n","for steps in range(10000): # a typical training loop\n","\n","  xb, yb = get_batch('train') # sample new batch of data\n","\n","  logits, loss = m(xb,yb) # evaluate loss\n","  optimizer.zero_grad(set_to_none=True) # zero out the gradients from previous steps\n","  loss.backward() # getting gradients for parameters\n","  optimizer.step() # use gradients to update parameters\n","\n","print(loss.item())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lxi4_RZvJzdz","executionInfo":{"status":"ok","timestamp":1691305150417,"user_tz":300,"elapsed":19974,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"e3614885-4d66-4289-cb2e-220e948df795"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["2.343282461166382\n"]}]},{"cell_type":"code","source":["print(decode(m.generate(torch.zeros((1,1), dtype=torch.long), max_new_tokens=500)[0].tolist())) # generation should be better after training, still poor because the model is only looking at one token for each prediction"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KjqezVuaLpM4","executionInfo":{"status":"ok","timestamp":1691305156022,"user_tz":300,"elapsed":139,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"4077dcb7-819e-4043-9b47-cbb1c974baba"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\tAn hiqY—Haveanvak Mo th thesthanut ses tre Je e tojze ng, And h,0:2\ty orofrdesssr Isal imus, whe trof mer wad m msacage h ansasuseandnooors bd, feshenn tovofo jupthethinthrhed f cortheved my this gas h mary makin o of l toganacche ul ld almin s acle aho w as he Sherd ck [Æ2:4\tAce hed h lllan And? on themy m onsreKe Uze s; hane, th.\n","Mall por, o o t ouigothouthr h kint an here f wn al:109\tBent 18:15:2:27\tTelaidrokes:1 Treusthesshrithin t t)5\tIs crel hesath hathe mblovexupererd caeo e ts hor di, in\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.nn import functional as F\n","\n","batch_size = 32\n","block_size = 8\n","max_iters = 3000\n","eval_interval = 300\n","learning_rate = 1e-2\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","eval_iters = 200\n","\n","with open('asv.txt', 'r', encoding='utf-8') as f:\n","    text = f.read()\n","\n","chars = sorted(list(set(text)))\n","vocab_size = len(chars)\n","\n","stoi = { ch:i for i,ch in enumerate(chars) } # mappings\n","itos = { i:ch for i,ch in enumerate(chars) }\n","encode = lambda s: [stoi[c] for c in s] # encodings\n","decode = lambda l: ''.join([itos[i] for i in l])\n","\n","data = torch.tensor(encode(text), dtype=torch.long)\n","n = int(0.9*len(data))\n","train_data = data[:n]\n","val_data = data[n:]\n","\n","def get_batch(split): # creates batch of input data (x) and target data (y)\n","    data = train_data if split == \"train\" else val_data\n","    ix = torch.randint(len(data) - block_size, (batch_size,))\n","    x = torch.stack([data[i:i+block_size] for i in ix])\n","    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n","    x, y = x.to(device), y.to(device)\n","    return x,y\n","\n","@torch.no_grad()\n","def estimate_loss():\n","    out = {}\n","    model.eval()\n","    for split in ['train', 'val']:\n","        losses = torch.zeros(eval_iters)\n","        for k in range(eval_iters):\n","            X, Y = get_batch(split)\n","            logits, loss = model(X,Y)\n","            losses[k] = loss.item()\n","        out[split] = losses.mean()\n","    model.train()\n","    return out\n","\n","\n","class BigramLanguageModel(nn.Module):\n","\n","    def __init__(self, vocab_size):\n","        super().__init__()\n","        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size) # look up table mapping tokens to the logits (probabilities) for the next token\n","\n","    def forward(self, idx, targets=None): # ends up getting called when the object is called\n","        # idx and targets are 2-dimensional (B,T) tensors\n","        logits = self.token_embedding_table(idx) # look up the logits for the next token, dimensions B,T,C (batch_size,time,channels/vocab_size)\n","\n","        if targets is None:\n","            loss = None\n","        else:\n","            B, T, C = logits.shape\n","            logits = logits.view(B*T, C) # reshape logits to 2-dimensions\n","            targets = targets.view(B*T) # reshape targets to 1-dimension\n","            loss = F.cross_entropy(logits, targets) # the quality of the predictions, expects C to be second dimension if multidimensional tensor\n","\n","        return logits, loss\n","\n","    def generate(self, idx, max_new_tokens):\n","        # idx is 2-dimensional (B,T) in this context\n","        for _ in range(max_new_tokens):\n","            logits, loss = self(idx) # get predictions\n","            logits = logits[:, -1, :] # look only at the last time step, becomes (B,C)\n","            probs = F.softmax(logits, dim=-1) # get probabilities\n","            idx_next = torch.multinomial(probs, num_samples=1) # sample the distribution for a single prediction in the time dimension in each batch dimension, (B,1)\n","            idx = torch.cat((idx, idx_next), dim=1) # append sample to the sequence, becomes (B,T+1)\n","        return idx\n","\n","\n","model = BigramLanguageModel(vocab_size)\n","m = model.to(device)\n","\n","optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)\n","\n","for iter in range(max_iters): # a typical training loop\n","\n","    if iter % eval_interval == 0:\n","        losses = estimate_loss()\n","        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n","\n","    xb, yb = get_batch('train') # sample new batch of data\n","\n","    logits, loss = m(xb,yb) # evaluate loss\n","    optimizer.zero_grad(set_to_none=True) # zero out the gradients from previous steps\n","    loss.backward() # getting gradients for parameters\n","    optimizer.step() # use gradients to update parameters\n","\n","print(loss.item())\n","\n","print(decode(m.generate(torch.zeros((1,1), dtype=torch.long, device=device), max_new_tokens=500)[0].tolist()))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zxACny34TxLU","executionInfo":{"status":"ok","timestamp":1691305177364,"user_tz":300,"elapsed":12405,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}},"outputId":"4b943ee9-9357-4fc2-f949-0e74d3db10c1"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["step 0: train loss 4.9292, val loss 4.9271\n","step 300: train loss 2.7475, val loss 2.7801\n","step 600: train loss 2.4281, val loss 2.4829\n","step 900: train loss 2.3742, val loss 2.4402\n","step 1200: train loss 2.3490, val loss 2.4147\n","step 1500: train loss 2.3315, val loss 2.4015\n","step 1800: train loss 2.3473, val loss 2.3975\n","step 2100: train loss 2.3339, val loss 2.4046\n","step 2400: train loss 2.3359, val loss 2.4077\n","step 2700: train loss 2.3354, val loss 2.3956\n","2.2291178703308105\n","\tAngothofof 1:4:36\tThed tuthig.\n","Je 14\tAndns 2238\tSe kiangat ars ar ayis, tches s, Je thang for sonerees 7\tJeret Ne woncor eid mocakerndncus t An 23: unid han gothevag aruch.\n","I to Gis oumin nyee it tinde tha o f amppon atherigheany, th am spwhe the avestowilaton 4\tAn ure:7:\n","Je trigoganclet sun m me h omeeses th f udel of An t ghee heserw anof I thwaeas, we mathay: it al he kof ubachentanlve 6:17\tAn ousea urey Se Ps 2:10\tIfinsthorehe kesheahe Ler t May, oht, as wias, ton he d thend alele th, an s J\n"]}]},{"cell_type":"markdown","source":["# Self Attention"],"metadata":{"id":"cGrLVvOYTrUj"}},{"cell_type":"code","source":["# toy example\n","torch.manual_seed(1337)\n","B,T,C = 4,8,2 # Batch, Time, Channels\n","x = torch.randn(B,T,C)\n","x.shape\n"],"metadata":{"id":"UBcxg081Ttg4","executionInfo":{"status":"aborted","timestamp":1691304961298,"user_tz":300,"elapsed":9,"user":{"displayName":"Rider Kirkpatrick","userId":"07723212021752168102"}}},"execution_count":null,"outputs":[]}]}